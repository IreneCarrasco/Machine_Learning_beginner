# Fuente principal del código fuente
# https://www.aprendemachinelearning.com/comprende-principal-component-analysis/

#importamos librerías
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')
from sklearn.decomposition import PCA # Importamos el módulo PCA 
from sklearn.preprocessing import StandardScaler # Importamos el módulo StandardScaler

#cargamos los datos de entrada
dataframe = pd.read_csv(r"comprar_alquilar.csv") 
print(dataframe.tail(10))

#normalizamos los datos
scaler=StandardScaler() # Para obtener los datos en forma de distribución normal. LLamo scalar a StandardScaler para que sea más breve llamar al método
df = dataframe.drop(['comprar'], axis=1) # quito la variable dependiente "Y"
scaler.fit(df) # calculo la media para poder hacer la transformacion
X_scaled=scaler.transform(df)# El método transform es el que normaliza los datos en forma de distribución normal

#Instanciamos objeto PCA y aplicamos
pca=PCA(n_components=9) # Otra opción es instanciar pca sólo con dimensiones nuevas hasta obtener un mínimo "explicado" ej.: pca=PCA(.85) (Varianza explicada)
pca.fit(X_scaled) # obtener los componentes principales
X_pca=pca.transform(X_scaled) # convertimos nuestros datos con las nuevas dimensiones de PCA

print("shape of X_pca", X_pca.shape)
expl = pca.explained_variance_ratio_ # Porcentaje de la varianza explicada para cada una de las componentes seleccionadas.
print(expl)
# expl trabaja con array, por lo que usamos []. En este caso cogemos las 6 primeras componentes
print('suma:',sum(expl[0:5]))
#Vemos que con 5 componentes tenemos algo mas del 85% de varianza explicada

#graficamos el acumulado de varianza explicada en las nuevas dimensiones
plt.plot(np.cumsum(pca.explained_variance_ratio_)) # np.cumsum devuelve la suma acumulada de los elementos a lo largo de un eje dado. Necesario que lleve asociado un array.
# En este caso nuestro array es el de las componentes pca.explained_variance_ratio_
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()

#graficamos en 2 Dimensiones, tomando los 2 primeros componentes principales

# Esta es una notación que se usa en Numpy y Pandas que vendría a ser [primera_fila_del_array:última_fila, de la columna que sea]
Xax=X_pca[:,0] # En este caso, coge todos los valores de la primera columna, de la primera a la última fila
Yax=X_pca[:,1] # Aquí cogemos todos los valores de la segunda columna

# Comprar es la variable dependiente del fichero csv que hemos cargado. Asignamos la variable labels a los valores de la columna comprar del dataframe
# Los valores de comprar son 0, que no quieren comprar, o 1, que significa que quieren comprar
labels=dataframe['comprar'].values
cdict={0:'red',1:'green'}
labl={0:'Alquilar',1:'Comprar'}
marker={0:'*',1:'o'}
alpha={0:.3, 1:.5}
fig,ax=plt.subplots(figsize=(7,5)) # Subplot que usamos en la última línea del bucle for que aparece a continuación
fig.patch.set_facecolor('white')
for l in np.unique(labels): # Encuentra los elementos únicos de un array, en este caso de labels 
    ix=np.where(labels==l) # Devuelve los elementos escogidos de x o y según una condición (aquí para que la condición es labels==1)
    ax.scatter(Xax[ix],Yax[ix],c=cdict[l],label=labl[l],s=40,marker=marker[l],alpha=alpha[l])
 
plt.xlabel("First Principal Component",fontsize=14)
plt.ylabel("Second Principal Component",fontsize=14)
plt.legend()
plt.show()
